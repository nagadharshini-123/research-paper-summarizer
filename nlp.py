# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QcBv9eDU15KCWNBaVnNZ4atsY675_7VU
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import PyPDF2
# from transformers import pipeline
# from langchain.text_splitter import CharacterTextSplitter
# 
# st.set_page_config(page_title="ðŸ“„ Research Paper Summarizer")
# 
# @st.cache_resource
# def load_model():
#     return pipeline("summarization", model="facebook/bart-large-cnn")
# 
# def extract_text_from_pdf(pdf_file):
#     reader = PyPDF2.PdfReader(pdf_file)
#     return "".join([page.extract_text() for page in reader.pages if page.extract_text()])
# 
# def split_text(text, chunk_size=1000, overlap=200):
#     splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
#     return splitter.split_text(text)
# 
# def summarize_text(chunks, model):
#     return [model(chunk, max_length=130, min_length=30, do_sample=False)[0]["summary_text"] for chunk in chunks]
# 
# st.title("ðŸ“„ Research Paper Summarizer")
# uploaded_file = st.file_uploader("Upload a research paper (PDF)", type=["pdf"])
# 
# if uploaded_file:
#     with st.spinner("Extracting text..."):
#         raw_text = extract_text_from_pdf(uploaded_file)
#     with st.spinner("Splitting text..."):
#         text_chunks = split_text(raw_text)
#     with st.spinner("Summarizing..."):
#         summarizer = load_model()
#         summaries = summarize_text(text_chunks, summarizer)
#     st.subheader("ðŸ“Œ Summary")
#     for i, summary in enumerate(summaries):
#         st.markdown(f"**Section {i+1}**: {summary}")
#

!pip install -q streamlit PyPDF2 transformers sentence-transformers langchain chromadb accelerate